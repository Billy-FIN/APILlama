{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a method to compare the similarity of the Json output between Llama 3, Llama 3 - one shot, GPT 3.5, and my model while dealing with API endpoints IE (information extraction) task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same test/eval data while in training\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('billyfin/doc2json')\n",
    "# delete the last line for future one-shot test\n",
    "one_shot_example = dataset['train'][166]\n",
    "dataset = dataset.filter(lambda example, idx: idx != 166, with_indices=True)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": \"MyIP.com JSON API Documentation\",\n",
      "    \"endpoints\": [\n",
      "        {\n",
      "            \"name\": \"Get IP Information\",\n",
      "            \"description\": \"Retrieves information about the IP address making the request.\",\n",
      "            \"method\": \"GET\",\n",
      "            \"url\": \"https://api.myip.com\",\n",
      "            \"headers\": [],\n",
      "            \"required_parameters\": [],\n",
      "            \"optional_parameters\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "JSON API | MyIP.com JSON API Contact JSON API You can make automated requests to the site using the API . Access URL: https://api.myip.com Response example: {\"ip\":\"66.249.75.9\",\"country\":\"United States\",\"cc\":\"US\"} Response elements: ip: IP address country: IP country location in English language cc: Two-letter country code in ISO 3166-1 alpha-2 format If there is no location data for an IP address cc will return \"XX\" and country \"Unknown\". Is this a free service? Yes. What are the API usage limits? There is no request limit, the only restriction is the server capacity which I will try to keep running smoothly. Can I use it for my commercial application? Yes, please give credit to myip.com if you can. How can I exclude some parameter(s) from the response? I will add that feature shortly, please check back for updates. I want you to add some feature, how can I contact you? If you have a suggestion please contact me here. MyIP.com 2024. × Contact For inquiries or feedback please get in touch at: OK\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset['json_form'][0])\n",
    "print(test_dataset['text_content'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the evaluation method for structure similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.75\n"
     ]
    }
   ],
   "source": [
    "def compare_and_score_json(json1, json2, path=\"\"):\n",
    "    \"\"\" Recursively compares two nested JSON objects and calculates similarity score. \"\"\"\n",
    "    total_score = 0\n",
    "    max_score = 0\n",
    "\n",
    "    if isinstance(json1, dict) and isinstance(json2, dict):\n",
    "        keys1 = set(json1.keys())\n",
    "        keys2 = set(json2.keys())\n",
    "        all_keys = keys1.union(keys2)\n",
    "\n",
    "        max_score += len(all_keys)\n",
    "\n",
    "        for key in all_keys:\n",
    "            if key in json1 and key in json2:\n",
    "                # Increment score for matching keys\n",
    "                sub_score, sub_max = compare_and_score_json(json1[key], json2[key], path + f\".{key}\")\n",
    "                total_score += sub_score\n",
    "                max_score += sub_max - 1  # Adjust because the key itself is counted in max_score\n",
    "            elif key in json1 or key in json2:\n",
    "                # Key missing in one of the JSONs\n",
    "                continue\n",
    "    elif isinstance(json1, list) and isinstance(json2, list):\n",
    "        # Simple comparison of list lengths; could be expanded to compare elements\n",
    "        min_len = min(len(json1), len(json2))\n",
    "        max_len = max(len(json1), len(json2))\n",
    "        total_score += min_len\n",
    "        max_score += max_len\n",
    "    # else:\n",
    "    #     # Compare scalar values\n",
    "    #     max_score += 1\n",
    "    #     if json1 == json2:\n",
    "    #         total_score += 1\n",
    "    #     elif type(json1) == type(json2):\n",
    "    #         # If types match but not values, give partial credit\n",
    "    #         total_score += 0.5\n",
    "\n",
    "    return total_score, max_score\n",
    "\n",
    "# Example JSON structures\n",
    "json1 = {\n",
    "    \"name\": \"Get IP Information\",\n",
    "    \"description\": \"Retrieve IP address information\",\n",
    "    \"details\": {\n",
    "        \"method\": \"GET\",\n",
    "        \"url\": \"https://api.myip.com\"\n",
    "    }\n",
    "}\n",
    "\n",
    "json2 = {\n",
    "    \"name\": \"Get IP Information\",\n",
    "    \"description\": \"Get the IP address, country, and two-letter country code.\",\n",
    "    \"details\": {\n",
    "        \"method\": \"POST\",  # Note the difference here\n",
    "        \"url\": \"https://api.myip.com\"\n",
    "    }\n",
    "}\n",
    "\n",
    "total_score, max_score = compare_and_score_json(json1, json2)\n",
    "similarity_score = total_score / max_score if max_score != 0 else 0\n",
    "print(f\"Similarity Score: {similarity_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the evaluation method for Json keys' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_similarity(json1, json2):\n",
    "    if isinstance(json1, dict) and isinstance(json2, dict):\n",
    "        common_keys = set(json1.keys()) & set(json2.keys())\n",
    "        total_similarity = sum(value_similarity(json1[k], json2[k]) for k in common_keys)\n",
    "        max_similarity = len(json1.keys()) + len(json2.keys()) - len(common_keys) # adjust for unmatched keys\n",
    "        return total_similarity / max_similarity if max_similarity else 1.0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def json_similarity_value(file1, file2):\n",
    "    with open(file1, 'r') as f1, open(file2, 'r') as f2:\n",
    "        json1 = json.load(f1)\n",
    "        json2 = json.load(f2)\n",
    "        return value_similarity(json1, json2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7214285714285713, 0.7214285714285713)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_strings(str1, str2):\n",
    "    \"\"\" Simple comparison for exact matches or partial based on common words. \"\"\"\n",
    "    if str1 == str2:\n",
    "        return 1.0\n",
    "    common_words = set(str1.split()) & set(str2.split())\n",
    "    return len(common_words) / max(len(str1.split()), len(str2.split()))\n",
    "\n",
    "def compare_responses(resp1, resp2):\n",
    "    \"\"\" Compare response elements by checking all keys and descriptions. \"\"\"\n",
    "    keys1 = set(resp1.keys())\n",
    "    keys2 = set(resp2.keys())\n",
    "    if keys1 != keys2:\n",
    "        return 0.5  # Partial score if keys do not match exactly\n",
    "    \n",
    "    # Full match of descriptions\n",
    "    for key in keys1:\n",
    "        if resp1[key] != resp2[key]:\n",
    "            return 0.75  # Slightly higher if only descriptions differ\n",
    "    return 1.0\n",
    "\n",
    "def compute(json, truth):\n",
    "    title_score = compare_strings(truth['title'], json['title']) * 0.05\n",
    "    \n",
    "    description_score = compare_strings(truth['description'], test_json['description']) * 0.15\n",
    "    method_score = (1 if ground_truth['method'] == test_json['method'] else 0) * 0.20\n",
    "    url_score = (1 if ground_truth['url'] == test_json['url'] else 0) * 0.20\n",
    "    response_score = compare_responses(ground_truth['response_elements'], test_response_elements) * 0.30\n",
    "    \n",
    "    total_score = name_score + description_score + method_score + url_score + response_score\n",
    "    total_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df1b1ad062d4e7da77945e1e7bd9d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModel, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup, BitsAndBytesConfig\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType, PeftModel, PeftConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "torch.manual_seed(42)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc70d8cc6a940cd93c98f066cf2c9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"quantization_config\": quantization_config},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "count = 1\n",
    "for test_sample in test_dataset['text_content']:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You will be given an API documentation. Extract the endpoints and output in JSON format.\"},\n",
    "        {\"role\": \"user\", \"content\": \"API text content: \" + test_sample + \"\\n\\nJson: \"},\n",
    "    ]\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    \n",
    "    result = outputs[0][\"generated_text\"]\n",
    "    with open(\"./model_outputs/llama3/\" + str(count) + \".txt\", 'w') as file:\n",
    "        file.write(result)\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3 - one shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"quantization_config\": quantization_config},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "count = 1\n",
    "for test_sample in test_dataset['text_content']:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"You will be given an API documentation. Extract the endpoints and output in JSON format.\\n\\nAPI text content: \" + one_shot_example['text_content'] + \"\\n\\nJson: \"},\n",
    "        {\"role\": \"assistant\", \"content\": one_shot_example['json_form']},\n",
    "        {\"role\": \"user\", \"content\": \"You will be given an API documentation. Extract the endpoints and output in JSON format.\\n\\nAPI text content: \" + test_sample + \"\\n\\nJson: \"},\n",
    "    ]\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    \n",
    "    result = outputs[0][\"generated_text\"]\n",
    "    with open(\"./model_outputs/llama3_one_shot/\" + str(count) + \".txt\", 'w') as file:\n",
    "        file.write(result)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT3.5 - one shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please type in your api key:  sk-None-LBwUJe7KgakZQCd1sFS2T3BlbkFJGZlBKtOqC13W19K504OG\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "OPENAI_API_KEY = str(input('Please type in your api key: '))\n",
    "\n",
    "count = 1\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "for test_sample in test_dataset['text_content']:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        # model=\"gpt-4-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"You will be given an API documentation. Extract the endpoints and output in JSON format.\\n\\nAPI text content: \" + one_shot_example['text_content'] + \"\\n\\nJson: \"},\n",
    "            {\"role\": \"assistant\", \"content\": one_shot_example['json_form']},\n",
    "            {\"role\": \"user\", \"content\": \"You will be given an API documentation. Extract the endpoints and output in JSON format.\\n\\nAPI text content: \" + test_sample + \"\\n\\nJson: \"},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    result = str(completion.choices[0].message.content)\n",
    "    with open(\"./model_outputs/gpt3.5_one_shot/\" + str(count) + \".txt\", 'w') as file:\n",
    "        file.write(result)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "Current CUDA Device: NVIDIA L40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "    current_device = torch.cuda.current_device()\n",
    "    device_name = torch.cuda.get_device_name(current_device)\n",
    "    print(\"Current CUDA Device:\", device_name)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75a2dcb0e7f40c4b2020fc60aa73e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a375c334bc4bce9f7609f8096321a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bb1f24452342b38b0abee42c50f38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/328k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_model_id = \"billyfin/llama_3_prompt_tuning_api2json_v4\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                            )\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct')\n",
    "\n",
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10240\n",
    "\n",
    "def format(example):\n",
    "    input_messages = [\n",
    "        {\"role\":\"user\", \"content\": one_shot_example['text_content']},\n",
    "        {\"role\":\"assistant\", \"content\": one_shot_example['json_form']},\n",
    "        {\"role\":\"user\", \"content\": example},\n",
    "    ]\n",
    "    example = tokenizer.apply_chat_template(input_messages, tokenize=False) + \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    return example\n",
    "    \n",
    "def preprocess_for_inference(examples):\n",
    "    inputs = f\"{examples}\"\n",
    "    \n",
    "    model_inputs = tokenizer(inputs)\n",
    "    model_inputs['input_ids'] += [tokenizer.pad_token_id]\n",
    "    model_inputs[\"attention_mask\"] = [1] * len(model_inputs[\"input_ids\"])\n",
    "    \n",
    "    sample_input_ids = model_inputs[\"input_ids\"]\n",
    "    model_inputs[\"input_ids\"] = [tokenizer.pad_token_id] * (\n",
    "        max_length - len(sample_input_ids)\n",
    "    ) + sample_input_ids\n",
    "    model_inputs[\"attention_mask\"] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "        \"attention_mask\"\n",
    "    ]\n",
    "    model_inputs[\"input_ids\"] = torch.tensor(model_inputs[\"input_ids\"][:max_length])\n",
    "    model_inputs[\"attention_mask\"] = torch.tensor(model_inputs[\"attention_mask\"][:max_length])\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/workspace/IE/venv/lib/python3.10/site-packages/peft/peft_model.py:1533: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (8192). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for test_sample in test_dataset['text_content']:\n",
    "    test_sample = format(test_sample)\n",
    "    test_input = preprocess_for_inference(test_sample)\n",
    "    inputs = {k: v.unsqueeze(0).to(device) for k, v in test_input.items()}\n",
    "    prompt = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"], \n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.1\n",
    "        )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0, prompt:], skip_special_tokens=True)\n",
    "    with open(\"./model_outputs/my_model/\" + str(count) + \".txt\", 'w') as file:\n",
    "        file.write(result)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28106\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset[24]['text_content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IE",
   "language": "python",
   "name": "ie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
